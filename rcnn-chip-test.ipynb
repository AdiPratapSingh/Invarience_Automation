{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#config\n\nimport torch\n\nBATCH_SIZE = 4 # increase / decrease according to GPU memeory\nRESIZE_TO = 512 # resize the image for training and transforms\nNUM_EPOCHS = 5 # number of epochs to train for\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# training images and XML files directory\n# ../input/microcontroller-detection/Microcontroller Detection\nTRAIN_DIR = '../input/microcontroller-test-data-notebook/Microcontroller Detection/train'\n# validation images and XML files directory\nVALID_DIR = '../input/microcontroller-test-data-notebook/Microcontroller Detection/test'\n\n# classes: 0 index is reserved for background\nCLASSES = [\n    'background', 'Arduino_Nano', 'ESP8266', 'Raspberry_Pi_3', 'Heltec_ESP32_Lora'\n]\nNUM_CLASSES = 5\n\n# whether to visualize images after crearing the data loaders\nVISUALIZE_TRANSFORMED_IMAGES = False\n\n# location to save model and plots\nOUT_DIR = './'\nSAVE_PLOTS_EPOCH = 2 # save loss plots after these many epochs\nSAVE_MODEL_EPOCH = 2 # save model after these many epochs","metadata":{"_uuid":"4f3f8abc-442d-4587-a76a-d1ee49dcab4f","_cell_guid":"d73e9133-cd27-4aa6-9ec3-d66b328e6a17","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-04T12:44:55.655063Z","iopub.execute_input":"2022-03-04T12:44:55.65559Z","iopub.status.idle":"2022-03-04T12:44:55.662424Z","shell.execute_reply.started":"2022-03-04T12:44:55.655555Z","shell.execute_reply":"2022-03-04T12:44:55.661664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utils\nimport albumentations as A\nimport cv2\nimport numpy as np\n\nfrom albumentations.pytorch import ToTensorV2\n# from config import DEVICE, CLASSES as classes\n\n# this class keeps track of the training and validation loss values...\n# ... and helps to get the average for each epoch as well\nclass Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n    \n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n    \n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\ndef collate_fn(batch):\n    \"\"\"\n    To handle the data loading as different images may have different number \n    of objects and to handle varying size tensors as well.\n    \"\"\"\n    return tuple(zip(*batch))\n\n# define the training tranforms\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        A.RandomRotate90(0.5),\n        A.MotionBlur(p=0.2),\n        A.MedianBlur(blur_limit=3, p=0.1),\n        A.Blur(blur_limit=3, p=0.1),\n        ToTensorV2(p=1.0),\n    ], bbox_params={\n        'format': 'pascal_voc',\n        'label_fields': ['labels']\n    })\n\n# define the validation transforms\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0),\n    ], bbox_params={\n        'format': 'pascal_voc', \n        'label_fields': ['labels']\n    })\n\n\ndef show_tranformed_image(train_loader):\n    \"\"\"\n    This function shows the transformed images from the `train_loader`.\n    Helps to check whether the tranformed images along with the corresponding\n    labels are correct or not.\n    Only runs if `VISUALIZE_TRANSFORMED_IMAGES = True` in config.py.\n    \"\"\"\n    if len(train_loader) > 0:\n        for i in range(1):\n            images, targets = next(iter(train_loader))\n            images = list(image.to(DEVICE) for image in images)\n            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n            sample = images[i].permute(1, 2, 0).cpu().numpy()\n            for box in boxes:\n                cv2.rectangle(sample,\n                            (box[0], box[1]),\n                            (box[2], box[3]),\n                            (0, 0, 255), 2)\n            cv2.imshow('Transformed image', sample)\n            cv2.waitKey(0)\n            cv2.destroyAllWindows()","metadata":{"_uuid":"1e59d1f3-8d79-4b1c-97aa-ad59fbd5c0c8","_cell_guid":"576cd00a-4af8-4f42-9ccb-9625d513baac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-04T12:44:58.109629Z","iopub.execute_input":"2022-03-04T12:44:58.110233Z","iopub.status.idle":"2022-03-04T12:45:00.453225Z","shell.execute_reply.started":"2022-03-04T12:44:58.110197Z","shell.execute_reply":"2022-03-04T12:45:00.452523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#datasets\n\nimport torch\nimport cv2\nimport numpy as np\nimport os\nimport glob as glob\n\nfrom xml.etree import ElementTree as et\n# from config import CLASSES, RESIZE_TO, TRAIN_DIR, VALID_DIR, BATCH_SIZE\nfrom torch.utils.data import Dataset, DataLoader\n# from utils import collate_fn, get_train_transform, get_valid_transform\n\n# the dataset class\nclass MicrocontrollerDataset(Dataset):\n    def __init__(self, dir_path, width, height, classes, transforms=None):\n        self.transforms = transforms\n        self.dir_path = dir_path\n        self.height = height\n        self.width = width\n        self.classes = classes\n        \n        # get all the image paths in sorted order\n        self.image_paths = glob.glob(f\"{self.dir_path}/*.jpg\")\n        self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n        self.all_images = sorted(self.all_images)\n\n    def __getitem__(self, idx):\n        # capture the image name and the full image path\n        image_name = self.all_images[idx]\n        image_path = os.path.join(self.dir_path, image_name)\n\n        # read the image\n        image = cv2.imread(image_path)\n        # convert BGR to RGB color format\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image_resized = cv2.resize(image, (self.width, self.height))\n        image_resized /= 255.0\n        \n        # capture the corresponding XML file for getting the annotations\n        annot_filename = image_name[:-4] + '.xml'\n        annot_file_path = os.path.join(self.dir_path, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # get the height and width of the image\n        image_width = image.shape[1]\n        image_height = image.shape[0]\n        \n        # box coordinates for xml files are extracted and corrected for image size given\n        for member in root.findall('object'):\n            # map the current object name to `classes` list to get...\n            # ... the label index and append to `labels` list\n            labels.append(self.classes.index(member.find('name').text))\n            \n            # xmin = left corner x-coordinates\n            xmin = int(member.find('bndbox').find('xmin').text)\n            # xmax = right corner x-coordinates\n            xmax = int(member.find('bndbox').find('xmax').text)\n            # ymin = left corner y-coordinates\n            ymin = int(member.find('bndbox').find('ymin').text)\n            # ymax = right corner y-coordinates\n            ymax = int(member.find('bndbox').find('ymax').text)\n            \n            # resize the bounding boxes according to the...\n            # ... desired `width`, `height`\n            xmin_final = (xmin/image_width)*self.width\n            xmax_final = (xmax/image_width)*self.width\n            ymin_final = (ymin/image_height)*self.height\n            yamx_final = (ymax/image_height)*self.height\n            \n            boxes.append([xmin_final, ymin_final, xmax_final, yamx_final])\n        \n        # bounding box to tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # area of the bounding boxes\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # no crowd instances\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        # labels to tensor\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        # prepare the final `target` dictionary\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n        # apply the image transforms\n        if self.transforms:\n            sample = self.transforms(image = image_resized,\n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            image_resized = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n        return image_resized, target\n\n    def __len__(self):\n        return len(self.all_images)\n\n# prepare the final datasets and data loaders\ntrain_dataset = MicrocontrollerDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform())\nvalid_dataset = MicrocontrollerDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=collate_fn\n)\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=collate_fn\n)\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n\n# # execute datasets.py using Python command from Terminal...\n# # ... to visualize sample images\n# # USAGE: python datasets.py\n# if __name__ == '__main__':\n#     # sanity check of the Dataset pipeline with sample visualization\n#     dataset = MicrocontrollerDataset(\n#         TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES\n#     )\n#     print(f\"Number of training images: {len(dataset)}\")\n    \n#     # function to visualize a single sample\n#     def visualize_sample(image, target):\n#         box = target['boxes'][0]\n#         label = CLASSES[target['labels']]\n#         cv2.rectangle(\n#             image, \n#             (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n#             (0, 255, 0), 2\n#         )\n#         cv2.putText(\n#             image, label, (int(box[0]), int(box[1]-5)), \n#             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n#         )\n#         cv2.imshow('Image', image)\n#         cv2.waitKey(0)\n        \n#     NUM_SAMPLES_TO_VISUALIZE = 5\n#     for i in range(NUM_SAMPLES_TO_VISUALIZE):\n#         image, target = dataset[i]\n#         visualize_sample(image, target)","metadata":{"_uuid":"78c75e59-4b06-46ae-b43e-0e9b045b60a6","_cell_guid":"5748cc13-264c-4c11-90ca-cf4ab9f66967","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-04T12:45:02.290733Z","iopub.execute_input":"2022-03-04T12:45:02.291342Z","iopub.status.idle":"2022-03-04T12:45:02.826688Z","shell.execute_reply.started":"2022-03-04T12:45:02.291293Z","shell.execute_reply":"2022-03-04T12:45:02.82579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\n\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndef create_model(num_classes):\n    \n    # load Faster RCNN pre-trained model\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    \n    # get the number of input features \n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # define a new head for the detector with required number of classes\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n\n    return model","metadata":{"_uuid":"3c9da3b4-3168-4de1-bc54-56a8d1d05e35","_cell_guid":"e291d61b-a6c8-4186-84d4-f41cc3041787","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-04T12:45:04.309996Z","iopub.execute_input":"2022-03-04T12:45:04.310704Z","iopub.status.idle":"2022-03-04T12:45:04.315348Z","shell.execute_reply.started":"2022-03-04T12:45:04.310668Z","shell.execute_reply":"2022-03-04T12:45:04.314542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#engine\n\n# from config import DEVICE, NUM_CLASSES, NUM_EPOCHS, OUT_DIR\n# from config import VISUALIZE_TRANSFORMED_IMAGES\n# from config import SAVE_PLOTS_EPOCH, SAVE_MODEL_EPOCH\n# from model import create_model\n# from utils import Averager\nfrom tqdm.auto import tqdm\n# from datasets import train_loader, valid_loader\n\nimport torch\nimport matplotlib.pyplot as plt\nimport time\n\nplt.style.use('ggplot')\n\n# function for running training iterations\ndef train(train_data_loader, model):\n    print('Training')\n    global train_itr\n    global train_loss_list\n    \n     # initialize tqdm progress bar\n    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n    \n    for i, data in enumerate(prog_bar):\n        optimizer.zero_grad()\n        images, targets = data\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        train_loss_list.append(loss_value)\n\n        train_loss_hist.send(loss_value)\n\n        losses.backward()\n        optimizer.step()\n\n        train_itr += 1\n    \n        # update the loss value beside the progress bar for each iteration\n        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n    return train_loss_list\n\n# function for running validation iterations\ndef validate(valid_data_loader, model):\n    print('Validating')\n    global val_itr\n    global val_loss_list\n    \n    # initialize tqdm progress bar\n    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n    \n    for i, data in enumerate(prog_bar):\n        images, targets = data\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n        \n        with torch.no_grad():\n            loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        val_loss_list.append(loss_value)\n\n        val_loss_hist.send(loss_value)\n\n        val_itr += 1\n\n        # update the loss value beside the progress bar for each iteration\n        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n    return val_loss_list\n\nif __name__ == '__main__':\n    # initialize the model and move to the computation device\n    model = create_model(num_classes=NUM_CLASSES)\n    model = model.to(DEVICE)\n    # get the model parameters\n    params = [p for p in model.parameters() if p.requires_grad]\n    # define the optimizer\n    optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n\n    # initialize the Averager class\n    train_loss_hist = Averager()\n    val_loss_hist = Averager()\n    train_itr = 1\n    val_itr = 1\n    # train and validation loss lists to store loss values of all...\n    # ... iterations till ena and plot graphs for all iterations\n    train_loss_list = []\n    val_loss_list = []\n\n    # name to save the trained model with\n    MODEL_NAME = 'model'\n\n    # whether to show transformed images from data loader or not\n    if VISUALIZE_TRANSFORMED_IMAGES:\n        from utils import show_tranformed_image\n        show_tranformed_image(train_loader)\n\n    # start the training epochs\n    for epoch in range(NUM_EPOCHS):\n        print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n\n        # reset the training and validation loss histories for the current epoch\n        train_loss_hist.reset()\n        val_loss_hist.reset()\n\n        # create two subplots, one for each, training and validation\n        figure_1, train_ax = plt.subplots()\n        figure_2, valid_ax = plt.subplots()\n\n        # start timer and carry out training and validation\n        start = time.time()\n        train_loss = train(train_loader, model)\n        val_loss = validate(valid_loader, model)\n        print(f\"Epoch #{epoch} train loss: {train_loss_hist.value:.3f}\")   \n        print(f\"Epoch #{epoch} validation loss: {val_loss_hist.value:.3f}\")   \n        end = time.time()\n        print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n\n        if (epoch+1) % SAVE_MODEL_EPOCH == 0: # save model after every n epochs\n            torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n            print('SAVING MODEL COMPLETE...\\n')\n        \n        if (epoch+1) % SAVE_PLOTS_EPOCH == 0: # save loss plots after n epochs\n            train_ax.plot(train_loss, color='blue')\n            train_ax.set_xlabel('iterations')\n            train_ax.set_ylabel('train loss')\n            valid_ax.plot(val_loss, color='red')\n            valid_ax.set_xlabel('iterations')\n            valid_ax.set_ylabel('validation loss')\n            figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n            figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n            print('SAVING PLOTS COMPLETE...')\n        \n        if (epoch+1) == NUM_EPOCHS: # save loss plots and model once at the end\n            train_ax.plot(train_loss, color='blue')\n            train_ax.set_xlabel('iterations')\n            train_ax.set_ylabel('train loss')\n            valid_ax.plot(val_loss, color='red')\n            valid_ax.set_xlabel('iterations')\n            valid_ax.set_ylabel('validation loss')\n            figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n            figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n\n            torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n        \n        plt.close('all')\n        # sleep for 5 seconds after each epoch\n        time.sleep(5)","metadata":{"_uuid":"0103448c-5220-4e36-b01f-8964eb1f00c8","_cell_guid":"8886eeb7-7000-43f5-a7e3-6091f0be9e10","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-04T12:45:05.836737Z","iopub.execute_input":"2022-03-04T12:45:05.837136Z","iopub.status.idle":"2022-03-04T12:47:08.861698Z","shell.execute_reply.started":"2022-03-04T12:45:05.837079Z","shell.execute_reply":"2022-03-04T12:47:08.860961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Inference\n\nimport numpy as np\nimport cv2\nimport torch\nimport glob as glob\nimport matplotlib.pyplot as plt\n%matplotlib inline\n! mkdir predictions\n# from model import create_model\n\n# set the computation device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# load the model and the trained weights\nmodel = create_model(num_classes=5).to(device)\nmodel.load_state_dict(torch.load(\n    './model5.pth', map_location=device\n))\nmodel.eval()\n\n# directory where all the images are present\nDIR_TEST = '../input/microcontroller-test-data-notebook/test_data'\ntest_images = glob.glob(f\"{DIR_TEST}/*\")\nprint(f\"Test instances: {len(test_images)}\")\n\n# classes: 0 index is reserved for background\nCLASSES = [\n    'background', 'Arduino_Nano', 'ESP8266', 'Raspberry_Pi_3', 'Heltec_ESP32_Lora'\n]\n\n# define the detection threshold...\n# ... any detection having score below this will be discarded\ndetection_threshold = 0.6\n\nfor i in range(len(test_images)):\n    # get the image file name for saving output later on\n    image_name = test_images[i].split('/')[-1].split('.')[0]\n    image = cv2.imread(test_images[i])\n    orig_image = image.copy()\n    # BGR to RGB\n    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n    # make the pixel range between 0 and 1\n    image /= 255.0\n    # bring color channels to front\n    image = np.transpose(image, (2, 0, 1)).astype(float)\n    # convert to tensor\n    image = torch.tensor(image, dtype=torch.float).cuda()\n    # add batch dimension\n    image = torch.unsqueeze(image, 0)\n    with torch.no_grad():\n        outputs = model(image)\n    \n    # load all detection to CPU for further operations\n    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n    # carry further only if there are detected boxes\n    if len(outputs[0]['boxes']) != 0:\n        boxes = outputs[0]['boxes'].data.numpy()\n        scores = outputs[0]['scores'].data.numpy()\n        # filter out boxes according to `detection_threshold`\n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        draw_boxes = boxes.copy()\n        # get all the predicited class names\n        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n        \n        # draw the bounding boxes and write the class name on top of it\n        for j, box in enumerate(draw_boxes):\n            cv2.rectangle(orig_image,\n                        (int(box[0]), int(box[1])),\n                        (int(box[2]), int(box[3])),\n                        (0, 0, 255), 2)\n            cv2.putText(orig_image, pred_classes[j], \n                        (int(box[0]), int(box[1]-5)),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n                        2, lineType=cv2.LINE_AA)\n\n        plt.imshow(orig_image)\n        cv2.waitKey(1)\n        cv2.imwrite(f\"{OUT_DIR}/predictions/{image_name}.jpg\", orig_image)\n#         (f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n    print(f\"Image {i+1} done...\")\n    print('-'*50)\n\nprint('TEST PREDICTIONS COMPLETE')\ncv2.destroyAllWindows()","metadata":{"_uuid":"ce653ad1-7fa2-4773-9e75-502e8529a328","_cell_guid":"83357768-5fce-477b-ab56-1fc2ae45b828","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-04T12:49:10.143877Z","iopub.execute_input":"2022-03-04T12:49:10.144166Z","iopub.status.idle":"2022-03-04T12:49:12.87774Z","shell.execute_reply.started":"2022-03-04T12:49:10.144135Z","shell.execute_reply":"2022-03-04T12:49:12.876932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}